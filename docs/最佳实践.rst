最佳实践
===================================

1. 忽略你不想要的任务结果
-----------------------------------

有时候我们并不关心任务执行的结果, 那么就可以ignore_result选项让任务忽略结果, 毕竟存储结果费时也费资源 ::

    @app.task(ignore_result=True)
    def mytask():
        something()
        
我们也可以通过全局配置选项task_ignore_result 来进行设置


2. 避免产生同步的子任务
-----------------------------------

让一个任务等待另一个任务的结果是非常效率低下的, 甚至当worker池耗尽的时候可能造成死锁

在设计任务时确保使用异步模式, 例如使用回调

**Bad** ::

    @app.task
    def update_page_info(url):
        page = fetch_page.delay(url).get()
        info = parse_page.delay(url, page).get()
        store_page_info.delay(url, info)

    @app.task
    def fetch_page(url):
        return myhttplib.get(url)

    @app.task
    def parse_page(url, page):
        return myparser.parse_document(page)

    @app.task
    def store_page_info(url, info):
        return PageInfo.objects.create(url, info)

**Good** ::

    def update_page_info(url):
        # fetch_page -> parse_page -> store_page
        chain = fetch_page.s(url) | parse_page.s() | store_page_info.s(url)
        chain()

    @app.task()
    def fetch_page(url):
        return myhttplib.get(url)

    @app.task()
    def parse_page(page):
        return myparser.parse_document(page)

    @app.task(ignore_result=True)
    def store_page_info(info, url):
        PageInfo.objects.create(url=url, info=info)
        
默认情况下, Celery会禁止以同步的方式运行子任务, 有些极端情况下你可能不得不这么做, 但是强烈建议你不这么做 ::

    @app.task
    def update_page_info(url):
        page = fetch_page.delay(url).get(disable_sync_subtasks=False)
        info = parse_page.delay(url, page).get(disable_sync_subtasks=False)
        store_page_info.delay(url, info)

    @app.task
    def fetch_page(url):
        return myhttplib.get(url)

    @app.task
    def parse_page(url, page):
        return myparser.parse_document(page)

    @app.task
    def store_page_info(url, info):
        return PageInfo.objects.create(url, info)


3. 性能和策略
-----------------------------------

任务粒度
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

我们通过每个子任务的计算量来评价任务的粒度, 通常来说我们最好是把任务切割成多个小的子任务, 而不是把任务设计 \
成少量的耗时久的任务. ::

    如果你的单个任务的代码达到数百行, 通常表明你的任务粒度太粗, 有优化的空间
    
通过小的任务, 我们可以并行处理更多的任务, 耗时久的任务会导致worker没有办法处理其他等待的任务.

然而, 执行任务是有额外开销的, 每次处理一个任务都需要发送一个消息, 另外数据可能不在本地等; 因此如果任务过于 \
细粒度, 增加的额外开销可能会抵消带来的好处 ::

    个人感觉和操作系统进程调度的时间片设计有点类似, 时间片太长, 可能导致部分进程长时间无法响应; 时间片太短,
    那么CPU耗费大量的时间用于进程切换, 越来越认同"计算机是一门妥协的艺术"这句话, 人生何尝不是呢!
    
数据位置
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

处理任务的worker应该尽可能接近数据, 如果数据在很远的地方, 你可以尝试在数据本地运行另一个worker, 或者缓存经 \
常使用的数据, 亦或是预加载你知道的将会用到的数据!

**worker之间共享数据最简单的方式就是使用分布式缓存系统, 例如memcached.**

状态
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

因为Celery是分布式系统, 因此你无法预知任务会被哪个进程或在哪个机器上执行, 你甚至无法了解任务是否会被及时执行. 

断言世界是任务的责任; 如果你有一个任务用于重建搜索引擎的索引, 而这个搜索引擎在每分钟内可能最多只能被重建索引 \
5次, 那么断言此事实就应该是任务的责任, 而不是调用者的.

另一些疑难杂症例如某些ORM(譬如Django自带的ORM)的model objects, 我们不应该把它们作为参数传递给任务, 最好是 \
在任务执行的时候重新从数据库获取, 否则可能产生一些竞态条件. 假设现在有个场景, 你写了一篇文章并使用一个任务 \
来自动扩展文章里的缩略语; 但是现在任务队列非常的繁忙, 因此你的任务可能在2分钟内都不会被执行; 同时, 另一个作 \
者修改了这篇文章; 那么当任务最终被执行时, 这篇文章会被恢复到之前的旧版本, 因为该任务的参数中存放了之前的body.

    class Article(models.Model):
        title = models.CharField()
        body = models.TextField()

    @app.task
    def expand_abbreviations(article):
        article.body.replace('MyCorp', 'My Corporation')
        article.save()

    >>> article = Article.objects.get(id=102)
    >>> expand_abbreviations.delay(article)
    
修正这个竞态条件是非常容易的, 使用文章id作为参数就可以了, 在任务的执行体内重新获取对应的文章: ::
    
    @app.task
    def expand_abbreviations(article_id):
        article = Article.objects.get(id=article_id)
        article.body.replace('MyCorp', 'My Corporation')
        article.save()
        
    >>> expand_abbreviations.delay(article_id)
    
数据库事务
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

以下是一个Django的视图, 我们首先在数据库里创建一篇文章, 然后将其主键作为参数传递给任务, 这里用到commit_on_success \
装饰器, 它会在视图返回时自动提交事务, 或者在触发异常时自动回滚. ::

    from django.db import transaction

    @transaction.commit_on_success
    def create_article(request):
        article = Article.objects.create()
        expand_abbreviations.delay(article.pk)

这里也存在一个竞态条件: 如果任务在事务被提交前开始执行, 那么其实在数据库中是获取不到这篇文章的(这里假设worker \
使用的数据库连接设置的事务隔离级别不是Read Uncommitted).

该问题的解决办法是使用on_commit回调来发送任务一旦所有事务被成功提交: ::

    from django.db.transaction import on_commit

    def create_article(request):
        article = Article.objects.create()
        on_commit(lambda: expand_abbreviations.delay(article.pk))